# -*- coding: utf-8 -*-
"""
Created on Mon May 17 12:02:43 2021

@author: jifon
"""

# ==================== CLASE DE MACHINE LEARNING - DATA SCIENCE ================================

# ======= HERRAMIENTAS ========
#pip install pandas
#pip install NumPy
#pip install matplotlib
#pip install IPython
#pip install scikit-learn
#pip install --upgrade tensorflow

# ======= RECOMENDACIONES ========
'''
Dentro de Anaconda s♠e puede ir a Environments y crear un nuevo entorno para trabajar.
Github: joanby, acá existe un repositorio llamado Collab, el que se puede importar desde la nube y usar servidores de Google.
Jupyter: dejar todo en la nube y ver ahí mismo el resultado del código (WebApp).
Se recomienda bajar Discord para ver el chat de Python, donde se divide en distintos temas el grupo.
'''

# ============== SECCIÓN 6 ==================
#Data Wrangling = Cirugía de Datos
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

RUTA_CLASE = "C:/Users/jfones/Desktop/Python - CS50/Anaconda/Curso UDEMY/Machine Learning Github/datasets"

data_ads = pd.read_csv(RUTA_CLASE + "/ads/Advertising.csv")

data_ads["corrn"] = (data_ads["TV"] - np.mean(data_ads["TV"]))*(data_ads["Sales"] - np.mean(data_ads["Sales"])) #CORRELACIÓN ENTRE TV Y VENTAS
data_ads["corr1"] = (data_ads["TV"] - np.mean(data_ads["TV"]))**2 #CORRELACIÓN DE LAS TVs
data_ads["corr2"] = (data_ads["Sales"] - np.mean(data_ads["Sales"]))**2 #CORRELACIÓN DE LAS VENTAS

corr_pearson = sum(data_ads["corrn"])/np.sqrt(sum(data_ads["corr1"])*sum(data_ads["corr2"]))
#COMO corr_pearson ES POSITIVO, ESTO SIGNIFICA QUE LA TV AYUDA EN MEJORAR LAS VENTAS

def corr_coef(df, var1, var2):
    df["corrn"] = (df[var1] - np.mean(df[var1]))*(df[var2] - np.mean(df[var2]))
    df["corr1"] = (df[var1] - np.mean(df[var1]))**2
    df["corr2"] = (df[var2] - np.mean(df[var2]))**2
    
    corr_pearson = sum(df["corrn"])/np.sqrt(sum(df["corr1"])*sum(df["corr2"]))
    return corr_pearson

corr_coef(data_ads, "TV", "Sales")

cols = data_ads.columns.values

for x in cols:
    for y in cols:
        print(x + ", " + y + ": " + str(corr_coef(data_ads, x , y)))
        
plt.plot(data_ads["TV"], data_ads["Sales"], "ro")
plt.title("Gastos en TV vs Ventas del Producto")

data_ads.corr() #ESTO ES LO MISMO QUE TODO EL CÁLCULO DE ARRIBA, PERO PANDAS YA LA TIENE INTEGRADA..
plt.matshow(data_ads.corr()) #VER MATRIZ DE CORRELACIÓN EN COLORES..


# ============== SECCIÓN 7 ==================
# MODELO DE REGRESIÓN LINEAL, Con datos simulados
# Para los valores X, serán 100 valores distribuidos según una N(1.5, 2.5) = N("Media", "Desv. Estándar")
# Para los valores Ye = 5 + 1.9 * X + e (error), donde error tendrá distribución N(0, 0.8)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

x = 1.5 + 2.5*np.random.randn(100) #randn es RANDOM NORMAL
res = 0 + 0.8*np.random.randn(100) #calcular el error con la distribución normal
y_pred = 5 + 1.9*x 
y_act = 5 + 1.9*x + res

x_list = x.tolist()
y_pred_list = y_pred.tolist()
y_act_list = y_act.tolist()

data = pd.DataFrame(
    {
     "x":x_list,
     "y_actual":y_act_list,
     "y_prediccion":y_pred_list
     }
)

data.head()

y_mean = [np.mean(y_act) for i in range(1, len(x_list)+1)]
data["SSR"] = (data["y_prediccion"] - np.mean(y_act))
data["SSD"] = (data["y_prediccion"] - y_act)
data["SST"] = (data["y_actual"] - np.mean(y_act))

SSR = sum(data["SSR"]**2)
SSD = sum(data["SSD"]**2)
SST = sum(data["SST"]**2)

R2 = SSR/SST

plt.plot(x,y_pred)
plt.plot(x,y_act, "ro")
plt.plot(x,y_mean, "g")
plt.title("Valor Actual vs Predicción")

#SSD: Diferencia entre el punto y la función lineal
#SST: Diferencia entre el punto y el promedio de los datos
#SSR: Diferencia entre la recta y el promedio de los datos
#SST = SSR + SSD
#R2 = SSR/SST

x_mean = np.mean(data["x"])
y_mean = np.mean(data["y_actual"])

data["beta_n"] = (data["x"] - x_mean)*(data["y_actual"]-y_mean) #COVARIANZA
data["beta_d"] = (data["x"] - x_mean)**2 #VARIANZA

beta = sum(data["beta_n"])/sum(data["beta_d"])
alpha = y_mean - beta*x_mean

data["y_model"] = alpha + beta*data["x"]

#RSE: Error estándar de los residuos, mientras menor sea, mejor es el modelo

RSE = np.sqrt(SSD/(len(data)-2))



# MODELO DE REGRESIÓN LINEAL CON DATOS CONOCIDOS
import pandas as pd
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
import numpy as np

data = pd.read_csv(RUTA_CLASE + "/ads/Advertising.csv")

lm = smf.ols(formula="Sales~TV", data = data).fit() #lm = Lineal Model
lm.params
#EL MODELO LINEAL PREDICTIVO SERÍA: Sales = 7.032594 (Valor Intercept de arriba) + 0.047537 (Valor TV de arrba) * TV

lm.pvalues
lm.rsquared #VALOR DE R2
lm.rsquared_adj #VALOR DE R2 AJUSTADO, HACE UN PEQUEÑO AJUSTE SEGÚN LA CANTIDAD DE ELEMENTOS
lm.summary() #RESUMEN DE TODA LA TABLA

sales_prd = lm.predict(pd.DataFrame(data["TV"])) #PREDICCIÓN DE VENTAS USANDO LOS COSTOS DE TV

data.plot(kind = "scatter", x = "TV", y = "Sales")
plt.plot(pd.DataFrame(data["TV"]), sales_prd, c="red", linewidth = 2)

data["sales_pred"] = 7.032594 + 0.047537*data["TV"]
data["RSE"] = (data["Sales"] - data["sales_pred"])**2
SSD = sum(data["RSE"])
RSE = np.sqrt(SSD/(len(data)-2))

sales_m = np.mean(data["Sales"])

error_promedio = RSE/sales_m #% DEL MODELO QUE NO QUEDA EXPLICADO..

plt.hist((data["Sales"] - data["sales_pred"])) #TIENE QUE TENER UNA DISTRIBUCIÓN NORMAL..




# MODELO DE REGRESIÓN LINEAL MÚLTIPLE
'''
Reglas
    - Para eliminar modelos que no aportan, hay que quitar los que tengan p-valor muy alto.
    - Otra manera, es buscar el que aumenten más el R2. Ver si las nuevas variables incrementan este valor.
    - Primero se elige la que tenga el menor valor residual.
    - Python ya tiene esto incluido en algunas librerías, pero primero se hará a mano.
'''

#AÑADIR NEWSPAPER AL MODELO EXISTENTE..
lm2 = smf.ols(formula="Sales~TV+Newspaper", data = data).fit()
lm2.params #Función: 5.774948 + 0.046901*TV + 0.044219*Newspaper
lm2.pvalues #A PESAR DE NO SER TAN PEQUEÑO EL DE NEWSPAPER, SIGUE SIENDO BASTANTE BAJO..
lm2.rsquared #ES MAYOR QUE EL R2 de lm (anterior)

sales_pred = lm2.predict(data[["TV", "Newspaper"]])
SSD = sum((data["Sales"] - sales_pred)**2)
RSE = np.sqrt(SSD/(len(data)-3)) #SE USA -3 PORQUE ES: VARIABLES PREDICTORAS + 1. RSE DISMINUYE UN POCO
error_model_2 = RSE/sales_m #QUEDA EN 22% APROX

lm2.summary() #PARA VER TODOS LOS DATOS..

#DE LO ANTERIOR, SE CONCLUYE QUE AÑADIR Newspaper BENEFICIA MUY POCO AL MODELO NUEVO

#VER CON LA RADIO..
lm3 = smf.ols(formula="Sales~TV+Radio", data = data).fit()
lm3.summary() #F-Statistic BAJA MUCHO (hasta elevado a -98) POR LO QUE DATOS DEBERÍAN SER MEJORES..
sales_pred_3 = lm3.predict(data[["TV", "Radio"]])
SSD = sum((data["Sales"] - sales_pred_3)**2)
RSE = np.sqrt(SSD/(len(data)-3)) #BAJA A CASI LA MITAD DEL ANTERIOR
error_model_3 = RSE/sales_m #QUEDA EN 12% APROX LO QUE NO SE PUEDE EXPLICAR..

#VER CON LAS 3 VARIABLES
lm4 = smf.ols(formula="Sales~TV+Radio+Newspaper", data = data).fit()
lm4.summary() #F-Statistic SUBE CON RESPECTO AL INTERIOR, INTERVALO DE CONFIANZA TOMA EL 0, y p-valor ES CASI 1. Coef NEGATIVO DEL Newspaper
sales_pred_4 = lm4.predict(data[["TV", "Radio", "Newspaper"]])
SSD = sum((data["Sales"] - sales_pred_4)**2)
RSE = np.sqrt(SSD/(len(data)-4)) #SUBE UN POCO
error_model_4 = RSE/sales_m #QUEDA SOBRE 12%, SUBE CON RESPECTO AL ANTERIOR



# == MULTICOLINEALIDAD ==
'''
MULTICOLINEALIDAD: VER CORRELACIÓN ENTRE UNA VARIABLE, EN COMPARACIÓN A LAS OTRAS 2 JUNTAS
  R2 VIF = 1/(1-R2) 
  VIF: Factor inflación de la varianza
- VIF = 1: Las variables no están correlacionadas
- VIF < 5: Las variables tienen una correlación moderada y se pueden quedar en el modelo
- VIF > 5: Las variables están altamente correlacionadas y deben desaparecer del modelo
'''

# Newspaper ~ TV + Radio
lm_n = smf.ols(formula="Newspaper~TV+Radio", data = data).fit()
rsquared_n = lm_n.rsquared
VIF_n = 1/(1-rsquared_n)

lm_tv = smf.ols(formula="TV~Newspaper+Radio", data = data).fit()
rsquared_tv = lm_tv.rsquared
VIF_tv = 1/(1-rsquared_tv)

lm_r = smf.ols(formula="Radio~Newspaper+TV", data = data).fit()
rsquared_r = lm_r.rsquared
VIF_r = 1/(1-rsquared_r)

#Newspaper y Radio tienen un VIF muy parecido, por lo que están correlacionadas.
#TV tiene un VIF igual a 1, lo que implica que no está correlacionada con las otras dos.


# == VALIDACIÓN DE MODELO ==
# Primero hay que dividir el dataset en conjunto de entrenamiento y de testing

a = np.random.randn(len(data))
check = (a<0.8)
training = data[check]
testing = data[~check]

#Sales = 3.0959 + 0.0461*TV + 0.1857*Radio (Va a cambiar al ser valores random)

# Validación con el conjunto de Testing
sales_pred = lm.predict(testing)

SSD = sum((testing["Sales"]-sales_pred)**2)
RSE = np.sqrt(SSD/(len(testing)-3))
sales_mean = np.mean(testing["Sales"])
error = RSE/sales_mean #ERROR DEL 17,3%



# ==== REGRESIÓN LINEAL CON SCIKIT ====
from sklearn.feature_selection import RFE
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np

data = pd.read_csv(RUTA_CLASE + "/ads/Advertising.csv")

feature_cols = ["TV", "Radio", "Newspaper"]
X = data[feature_cols]
Y = data["Sales"]

estimator = SVR(kernel="linear") #SE LE INDICA EL TIPO DE MODELO QUE SE QUIERE CREAR
selector = RFE(estimator, 2, step=1) #El 2 es para elegir 2 variables, y step la cantida de pasos para hacerlo
selector = selector.fit(X,Y)
selector.support_ #INDICA LAS VARIABLES SELECCIONADAS DE feature_cols
selector.ranking_  #PONE LAS ELEGIDAS Y QUIENES LES SIGUEN EN ORDEN

X_pred = X[["TV", "Radio"]]
lm = LinearRegression()
lm.fit(X_pred, Y)
lm.intercept_
lm.coef_
lm.score(X_pred,Y) #VALOR DE R2


# == TRABAJAR CON VARIABLES CATEGÓRICAS ==
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

data = pd.read_csv(RUTA_CLASE + "/ecom-expense/Ecom Expense.csv")

dummy_gender = pd.get_dummies(data["Gender"], prefix = "Gender") #Separa en un nuevo dataframe la variable elegida
dummy_city = pd.get_dummies(data["City Tier"], prefix = "City") #prefix es para darle el título el comienzo y queda con _ la variable

column_names = data.columns.values.tolist()

df_new = data[column_names].join(dummy_gender)
column_names = df_new.columns.values.tolist()

df_new = df_new[column_names].join(dummy_city)

feature_cols = ["Monthly Income", "Transaction Time", "Gender_Female", "Gender_Male", "City_Tier 1", "City_Tier 2", "City_Tier 3"]
#SI ES QUE SE AÑADE LA VARIABLE "Record" LOS DATOS CAMBIAN, R2 SUBE HASTA 0,92. SE PUEDE PROBAR CON LAS DISTINTAS COLUMNAS
X = df_new[feature_cols]
Y = df_new["Total Spend"]
lm = LinearRegression()
lm.fit(X,Y)

list(zip(feature_cols, lm.coef_))
lm.score(X,Y) #R2 de 0,194, BAJO - Falta información

#La función quedaría de la siguiente manera: intercept + coefs*variables_elegidas

df_new["prediction"] = "La función de arriba.. pero indicando la columna del dataframe" #intercept + df["col1"]*coef1 + df["col2"]*coef2....

SSD = np.sum((df_new["prediction"] - df_new["Total Spend"])**2)
RSE = np.sqrt(SSD/(len(df_new)-len(feature_cols)-1))
sales_mean = np.mean(df_new("Total Spend"))
error = RSE/sales_mean #DEBERÍA ENTREGAR UN ERROR DEL 13%

df_new["prediction"] = lm.predict(pd.DataFrame(df_new[feature_cols])) #OTRA MANERA DE ESCRIBIR LA FÓRMULA DE ARRIBA, MÁS FÁCIL..


# ELIMINAR VARIABLES DUMMIES REDUNDANTES

dummy_gender = pd.get_dummies(data["Gender"], prefix = "Gender").iloc[:,1:]
dummy_city = pd.get_dummies(data["City Tier"], prefix = "City").iloc[:,1:]
df_new = data[column_names].join(dummy_gender)
column_names = df_new[column_names].values.tolist()
df_new = df_new[column_names].join(dummy_city)

feature_cols = ["Monthly Income", "Transaction Time", "Gender_Male", "City_Tier 2", "City_Tier 3", "Record"]
X = df_new[feature_cols]
Y = df_new["Total Spend"]
lm = LinearRegression()
lm.fit(X,Y)



# = TRANSFORMACIÓN DE VARIABLES PARA CONSEGUIR UNA RELACIÓN NO LINEAL =
import pandas as pd
import matplotlib.pyplot as plt

data_auto = pd.read_csv(RUTA_CLASE + "/auto/auto-mpg.csv")
data_auto["mpg"] = data_auto["mpg"].dropna()
data_auto["horsepower"] = data_auto["horsepower"].dropna()
plt.plot(data_auto["horsepower"], data_auto["mpg"], "ro")
plt.xlabel("Caballos de Potencia")
plt.ylabel("Consumo (millas por galeón)")
plt.title("CV vs MPG")

#Modelo de regresión lineal: mpg = a + b*horsepower

X = data_auto["horsepower"].fillna(data_auto["horsepower"].mean())
Y = data_auto["mpg"].fillna(data_auto["mpg"].mean())

lm = LinearRegression()
lm.fit(X[:, np.newaxis],Y) #Lo de la X se agrega porque LinearRegression espera una matriz (bidimensional por lo menos)
X_data = X[:, np.newaxis]

plt.plot(X,Y, "ro")
plt.plot(X, lm.predict(X_data), color = "blue")
lm.score(X_data,Y)
SSD = np.sum((Y - lm.predict(X_data))**2)
RSE = np.sqrt(SSD/(len(X_data)-1))
y_mean = np.mean(Y)
error = RSE/y_mean
print(SSD, RSE, error)


# = MODELO DE REGRESIÓN CUADRÁTICO =
# mpg = a + b*horsepower^2

X_data = X**2
X_data = X_data[:, np.newaxis]
lm = LinearRegression()
lm.fit(X_data,Y)
lm.score(X_data,Y)
SSD = np.sum((Y - lm.predict(X_data))**2)
RSE = np.sqrt(SSD/(len(X_data)-1))
y_mean = np.mean(Y)
error = RSE/y_mean
print(SSD, RSE, y_mean, error) #R2 ES MENOR Y error ES MAYOR, POR LO QUE NO FUNCIONA PARA ESTA ESTIMACIÓN

# = MODELO DE REGRESIÓN LINEAL Y CUADRÁTICO =
# mpg = a + b*horsepower + c*horsepower^2

from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model

poly = PolynomialFeatures(degree=2)
X_data = poly.fit_transform(X[:,np.newaxis])
lm = linear_model.LinearRegression()
lm.fit(X_data, Y)
lm.score(X_data,Y) #R2 AUMENTA EN COMPARACIÓN A LOS MODELOS ANTERIORES
lm.intercept_ #Resultado: 55.026192447080355
lm.coef_ #Resultado: b = -0.43404318, c = 0.00112615

for d in range(2,6):
    poly = PolynomialFeatures(degree=d)
    X_data = poly.fit_transform(X[:,np.newaxis])
    lm = linear_model.LinearRegression()
    lm.fit(X_data, Y)
    print(lm.score(X_data, Y))
    
    
#EL PROBLEMA DE LOS OUTLIERS
X = data_auto["displacement"].fillna(data_auto["displacement"].mean())
X = X[:,np.newaxis]
Y = data_auto["mpg"].fillna(data_auto["mpg"].mean())
lm = LinearRegression()
lm.fit(X,Y)
lm.score(X,Y)
plt.plot(X,Y, "ro")
plt.plot(X, lm.predict(X), color = "blue")
data_auto[(data_auto["displacement"] > 250) & (data_auto["mpg"] > 35)]

data_auto_clean = data_auto.drop([395, 358, 395, 372]) #INDICES QUE SE SACAN POR SER OUTLIER, SE VE AL HACER LOS FILTROS Y VIENDO EL GRÁFICO

'''
Hay maneras de eliminar variables que tengan mucho peso en la función, para esto se puede calcular el APALANCAMIENTO.
Leverage_i = 1/n + ( (x_i - x_prom)^2 / sumatoria(x_i - x_prom)^2 )
Se eliminan las filas que tengan un mayor nivel de apalancamiento
'''




# ============== SECCIÓN 8 ==================

# == REGRESIÓN LOGÍSTICA ==

import pandas as pd

RUTA_CLASE = "C:/Users/jfones/Desktop/Python - CS50/Anaconda/Curso UDEMY/Machine Learning Github/datasets"

df = pd.read_csv(RUTA_CLASE + "/gender-purchase/Gender Purchase.csv")

contingency_table = pd.crosstab(df["Gender"], df["Purchase"])
contingency_table.sum(axis = 1) #SUMAR VALORES DE LA FILA
contingency_table.sum(axis = 0) #SUMAR VALORES POR LA COLUMNA
contingency_table.astype("float").div(contingency_table.sum(axis = 1), axis = 0)

# Regresión desde 0..

def likelihood(y, pi):
    import numpy as np
    total_sum = 1
    sum_in = list(range(1, len(y)+1))
    for i in range(len(y)):
        sum_in[i] = np.where(y[i] == 1, pi[i], 1-pi[i])
        total_sum = total_sum * sum_in[i]
        return total_sum

#CALCULAR LAS PROBABILIDADES PARA CADA OBSERVACIÓN
# P_i = P(X_i) = 1/ (1 + e^(-sum{j=0 hasta k} beta_j*X_ij))
def logitprobs(X, beta):
    import numpy as np
    n_rows = np.shape(X)[0]
    n_cols = np.shape(X)[1]
    pi = list(range(1, n_rows+1))
    expon = list(range(1, n_rows+1))
    for i in range(n_rows):
        expon[i] = 0
        for j in range(n_cols):
            ex = X[i][j]*beta[j]
            expon[i] = ex + expon[i]
        with np.errstate(divide="ignore", invalid = "ignore"):
            pi[i] = 1/(1+np.exp(-expon[i]))
    return pi

# CALCULAR LA MATRIZ DIAGONAL W
# diag(P_i * (1- P_i)){i = 1 hasta n}
def findW(pi):
    import numpy as np
    n = len(pi)
    W = np.zeros(n*n).reshape(n,n)
    for i in range(n):
        print(i)
        W[i,i] = pi[i]*(1-pi[i])
        W[i,i].astype(float)
    return W

# OBTENER LA SOLUCIÓN DE LA FUNCIÓN LOGÍSTICA
# beta_n+1 = beta_n - f(beta_n)/f'(beta_n)

def logistics(X,Y, limit):
    import numpy as np
    from numpy import linalg
    nrow = np.shape(X)[0]
    bias = np.ones(nrow).reshape(nrow,1)
    X_new = np.append(X, bias, axis = 1)
    ncol = np.shape(X_new)[1]
    beta = np.zeros(ncol).reshape(ncol,1)
    root_dif = np.array(range(1, ncol+1)).reshape(ncol,1)
    iter_i = 10000
    while(iter_i > limit):
        print(str(iter_i) + "," + str(limit))
        pi = logitprobs(X_new, beta)
        print(pi)
        W = findW(pi)
        print(W)
        num = (np.transpose(np.matrix(X_new))*np.matrix(Y - np.transpose(pi)).transpose())
        den = (np.matrix(np.transpose(X_new))*np.matrix(W)*np.matrix(X_new))
        root_dif = np.array(linalg.inv(den)*num)
        beta = beta + root_dif
        print(beta)
        print(iter_i)
        iter_i = np.sum(root_dif*root_dif)
        ll = likelihood(Y, pi)
    return beta

# COMPROBACIÓN DEL EXPERIMENTO..
import numpy as np
X = np.array(range(10)).reshape(10,1)
Y = [0,0,0,0,1,0,1,0,1,1]
bias = np.ones(10).reshape(10,1)
X_new = np.append(X,bias,axis=1)

a = logistics(X,Y,0.00001)

ll = likelihood(Y, logitprobs(X,a))

#Lo anterior devuelve la función Y = 0.66220827 + X -3.69557172

#USANDO STATSMODELS DE PYTHON QUEDA LO SIGUIENTE:
import statsmodels.api as sm
logit_model = sm.Logit(Y,X_new)
result = logit_model.fit()
print(result.summary2()) #Coef de x1 da 0.6622, igual que el modelo anterior. Mientras que la const también se parece.



#ANÁLISIS DE DATOS CON REGRESIÓN LOGÍSTICA PARA PREDICCIONES BANCARIAS:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

RUTA_CLASE = "C:/Users/jfones/Desktop/Python - CS50/Anaconda/Curso UDEMY/Machine Learning Github/datasets"
df = pd.read_csv(RUTA_CLASE + "/bank/bank.csv", sep = ";")

df["y"] = (df["y"] == "yes").astype(int) #PASAR LOS "yes" A 1 Y LOS "no" A 0
df["education"].unique() #PARA VER LAS DISTINTAS CLASIFICACIONES QUE HAY EN LA COLUMNA "education"

df["education"] = np.where(df["education"] == "basic.4y", "Basic", df["education"])
df["education"] = np.where(df["education"] == "basic.6y", "Basic", df["education"])
df["education"] = np.where(df["education"] == "basic.9y", "Basic", df["education"])

df["education"] = np.where(df["education"] == "high.school", "High School", df["education"])
df["education"] = np.where(df["education"] == "professional.course", "Professional Course", df["education"])
df["education"] = np.where(df["education"] == "university.degree", "University Degree", df["education"])

df["education"] = np.where(df["education"] == "illiterate", "Illiterate", df["education"])
df["education"] = np.where(df["education"] == "unknown", "Unknown", df["education"])

df["y"].value_counts() #3.668 personas no comprar el producto, mientras que 451 sí lo hicieron.

df.groupby("y").mean() #PARA VER LOS PROMEDIOS DE ACUERDO A LA GENTE QUE COMPRÓ O NO EL CRÉDITO
df.groupby("education").mean() #VER PROMEDIOS POR NIVEL DE EDUCACIÓN

# = HISTOGRAMA ENTRE COMPRAS Y NIVEL DE EDUCACIÓN =
pd.crosstab(df.education, df.y).plot(kind="bar")
plt.title("Frecuencia de compra en función del nivel de educación")
plt.xlabel("Nivel de educación")
plt.ylabel("Frecuencia de compra del producto")
# SOLO VIENDO EL GRÁFICO, PARECIERA QUE EL NIVEL DE EDUCACIÓN SÍ IMPORTA EN LA DECISIÓN DE COMPRA..

# = TABLA DE DATOS (APILADO) PARA VER EL % DE LAS PERSONAS CASADAS QUE COMPRAN O NO EL PRODUCTO =
table = pd.crosstab(df.marital, df.y)
table.div(table.sum(1).astype(float), axis = 0).plot(kind="bar", stacked = True)
plt.title("Diagrama apilado de estado civil contra el nivel de compras")
plt.xlabel("Estado Civil")
plt.ylabel("Proporción de Clientes")
# PARECIERA QUE NO IMPORTA MUCHO SI ESTÁN CASADOS O NO..

# = FRECUENCIA DE COMPRA SEGÚN EL MES = TAMBIÉN SE PUEDE VER PARA EL DÍA DE LA SEMANA (AUNQUE PARECIERA QUE NO INFLUYE)
pd.crosstab(df.month, df.y).plot(kind="bar")
plt.title("Frecuencia de compra en función del día de la semana")
plt.xlabel("Mes del Año")
plt.ylabel("Frecuencia de compra del producto")

# = CANTIDAD DE CLIENTES SEGÚN LA EDAD
df.age.hist()
plt.title("Histograma de la Edad")
plt.xlabel("Edad")
plt.ylabel("Cliente")

#PARA VER POR EDAD SI ES QUE COMPRARON O NO. DONDE PARECIERA QUE GENTE MÁS JOVEN SÍ INVIERTE MÁS..
pd.crosstab(df.age, df.y).plot(kind="bar")

#PARA VER SI EN CAMPAÑAS ANTERIORES DIJERON QUE SÍ O NO, Y QUÉ DIJERON AHORA
pd.crosstab(df.poutcome, df.y).plot(kind="bar")


# = CONVERSIÓN DE LAS VARIABLES CATEGÓRICAS A DUMMIES =
categories = ["job", "marital", "education", "housing", "loan", "contact", "month", "day_of_week", "poutcome"]

for category in categories:
    cat_list = "cat_" + category
    cat_dummies = pd.get_dummies(df[category], prefix = category)
    data_new = df.join(cat_dummies)
    df = data_new

df_vars = df.columns.values.tolist()
to_keep = (v for v in df_vars if v not in categories)
to_keep = (v for v in df_vars if v not in ["default"])
bank_data = df[to_keep]
bank_data.columns.values

bank_data_vars = bank_data.columns.values.tolist()
Y = ['y']
X = [v for v in bank_data_vars if v not in Y]


#SELECCIÓN DE RASGOS PARA EL MODELO
n = 12

from sklearn import datasets
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
rfe = RFE(lr, n)
rfe = rfe.fit(bank_data[X], bank_data[Y].values.ravel())

print(rfe.support_) #PARA VER LOS QUE QUEDAN DENTRO
print(rfe.ranking_) #LAS QUE SON 1 SON LAS QUE QUEREMOS QUEDAR (DEBERÍAN SER 12 PORQUE ELEGIMOS ESTA CANTIDAD..)

list(zip(bank_data_vars,rfe.support_,rfe.ranking_)) #VER LOS NOMBRES DE LAS COLUMNAS, SI ES QUE FUE ELEGIDA O NO Y EL RANKING..

cols = ["euribor3m", "job_admin.", "job_blue-collar", "job_housemaid", "job_retired", "loan_yes", "month_aug", "month_dec",
        "month_jul", "month_jun", "month_mar", "poutcome_nonexistent"]

X = bank_data[cols]
Y = bank_data["y"]


# == IMPLEMENTAR LA REGRESIÓN LOGÍSTICA ==

import statsmodels.api as sm
logit_model = sm.Logit(Y,X)
result = logit_model.fit()
result.summary2() #Variable Dependiente ES LA QUE QUEREMOS PREDECIR (y). DF RESIDUAL ES #OBSERVACIONES - DATOS -1. REVISAR LOS pvalor (P>|z|), MIENTRAS MÁS PEQUEÑO SEA, MEJOR.

# = IMPLEMENTAR SCIKIT LEARN PARA AJUSTAR EL MODELO =
from sklearn import linear_model

logit_model = linear_model.LogisticRegression()
logit_model.fit(X,Y)

logit_model.score(X,Y) #R2 ES DE 0,8934 POR LO QUE ES MUY ELEVADO Y DEBERÍAMOS TENER UNA BUENA PREDICCIÓN
Y.mean() #PROMEDIO DE GENTE QUE COMPRA, QUE ES ALREDEDOR DEL 11% Y NUESTRO MODELO ES UN POCO MEJOR QUE EL PROMEDIO

pd.DataFrame(list(zip(X.columns, np.transpose(logit_model.coef_)))) #PONER NOMBRE DE LA COLUMNA CON SU COEFICIENTE CORRESPONDIENTE


from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state=0) #RANGO DEL 30%
lm = linear_model.LogisticRegression()
lm.fit(X_train, Y_train)

probs = lm.predict_proba(X_test) #SI LA COLUMNA 2 ES MAYOR A 0.5 SE CATALOGA COMO COMPRA, Y EN CASO CONTRARIO, COMO NO COMPRA..

prediction = lm.predict(X_test) #PONE UN 0 EN EL CASO DE QUE LA PROBABILIDAD SEA MENOR A 0.5

prob = probs[:,1]
prob_df = pd.DataFrame(prob)
threshold = 0.1 #"epsilon" PARA ELEGIR EL NIVEL DE PROBABILIDAD MÍNIMO QUE VAMOS A EXIGIR, SE TOMA 0.1 YA QUE 10% COMPRA..
prob_df["prediction"] = np.where(prob_df[0]>threshold, 1, 0) #SE CREA UNA COLUMNA DONDE CUMPLE SI LA PROBABILIDAD ES MAYOR A 0.1 (10%)
prob_df.head()
pd.crosstab(prob_df.prediction, columns="count") #390 POSIBLES COMPRADORES vs 846 QUE NO.. ESTO ES ALREDEDOR DE UN 30%

threshold = 0.15 #LA IDEA ES TOMAR VARIOS ESCENARIOS PARA IR VIENDO EL MOVIMIENTO
prob_df["prediction"] = np.where(prob_df[0]>threshold, 1, 0) #SE CREA UNA COLUMNA DONDE CUMPLE SI LA PROBABILIDAD ES MAYOR A 0.1 (10%)
prob_df.head()
pd.crosstab(prob_df.prediction, columns="count") #DISMINUYE A 312, DONDE 924 NO COMPRARÍAN..

#TOMANDO UN THRESHOLD MENOR, PASARÍA LO CONTRARIO..

from sklearn import metrics
metrics.accuracy_score(Y_test, prediction) #ESTO DICE LA CANTIDAD DE CASOS QUE ACERTAMOS EN LA PREDICCIÓN..


# == VALIDACIÓN CRUZADA ==
from sklearn.model_selection import cross_val_score

scores = cross_val_score(linear_model.LogisticRegression(), X, Y, scoring="accuracy", cv = 10)
scores.mean() #SE OBTIENE ALREDEDOR DE UN 90%, MUY PARECIDO A LA VALIDACIÓN ANTERIOR.

# IMPLEMENTACIÓN DE LAS CURVAS ROC

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3, random_state=0)
lm = linear_model.LogisticRegression()
lm.fit(X_train, Y_train)

probs = lm.predict_proba(X_test)
prob = probs[:,1]
prob_df = pd.DataFrame(prob)
threshold = 0.1
prob_df["prediction"] = np.where(prob_df[0] >= threshold, 1, 0)
prob_df["actual"] = list(Y_test)

confusion_matrix = pd.crosstab(prob_df.prediction, prob_df.actual) #Tabla se lee: Negativos Verdaderos |  Falsos Verdaderos / Negativos Falsos | Positivos Verdaderos
TN = confusion_matrix[0][0]
TP = confusion_matrix[1][1]
FP = confusion_matrix[0][1]
FN = confusion_matrix[1][0]

sens = TP/(TP+FN) #Sensibilidad del 21%
espc_1 = 1-TN/(TN+FP) #Especifidad es del 4,7%



threshold = [0.04, 0.05, 0.07, 0.1, 0.12, 0.15, 0.18, 0.2, 0.25]
sensitivities = [] #SE PUEDE AGREGAR EL VALOR 1 PARA QUE TERMINE EN ESTE VALOR
especifities_1 = []

for t in threshold:
    prob_df["prediction"] = np.where(prob_df[0] >= t, 1, 0)
    prob_df["actual"] = list(Y_test)
    confusion_matrix = pd.crosstab(prob_df.prediction, prob_df.actual)
    TN = confusion_matrix[0][0]
    TP = confusion_matrix[1][1]
    FN = confusion_matrix[0][1]
    FP = confusion_matrix[1][0]
    sens = TP/(TP+FN)
    sensitivities.append(sens)
    espc_1 = 1-TN/(TN+FP)
    especifities_1.append(espc_1)

#sensitivities.append(0) ESTO ES PARA QUE COMIENCE CON EL 0 EN EL GRÁFICO
#especifities_1.append(0)

import matplotlib.pyplot as plt
plt.plot(especifities_1, sensitivities, marker="o", linestyle = "--", color = "r")
x = (i*0.01 for i in range(100))
y = (i*0.01 for i in range(100))
plt.plot(x,y)
plt.xlabel("1-Especifidad")
plt.ylabel("Sensibilidad")
plt.title("Curva ROC")

#Curva ROC tiene que estar por arriba de la diagonal, ya que esto significa que la probabilidad es mejor que un 50-50. Si está por abajo, el modelo es muy malo..
#Mientras mayor sea el área de la curva ROC, mejor es el modelo..

from sklearn import metrics
from ggplot import * #HAY QUE INSTALAR ggplot. pip install ggplot

espc_1, sensit, _ = metrics.roc_curve(Y_test, prob) #Sensibilidad lo hace desde 0.01, sumando este mismo número hasta 1

df = pd.DataFrame({
    "esp": espc_1,
    "sens": sensit
    })

ggplot(df, aes(x="esp", y = "sens")) + geom_line() + geom_abline(lineatype = "dashed") + xlim(-0.01, 1.01) + ylim(-0.01, 1.01) + xlabel("1-Especifidad") + ylabel("Otra cosa..")

auc = metrics.auc(espc_1, sensit) #ESTO ES PARA CALCULAR EL ÁREA BAJO LA CURVA

ggplot(df, aes(x="esp", y="sens")) + geom_area(alpha=0.25) + geom_line(aes(y="sens")) + ggtitle("Curva ROC y AUC=%s" & str(auc))






# ============== SECCIÓN 9 ==================

# == CLUSTERING ==
'''
    - Clustering: conjunto de datos o grupos homogeneos.
    - Distancias a veces son engañosas, es mejor si se normalizan los datos para que tengan el mismo peso.
    - La idea es separar en distintos grupos que se parezcan, o tengan una distancia parecida.
'''
from scipy.spatial import distance_matrix
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

RUTA_CLASE_HOME = "D:/Python/Github/Curso UDemy/python-ml-course/datasets"
data = pd.read_csv(RUTA_CLASE_HOME + "/movies/movies.csv", sep = ";")

movies = data.columns.values.tolist()[1:] #No considero la primera columna porque no nos sirve para el ejercicio

dd1 = distance_matrix(data[movies], data[movies], p=1) #Se hacen los distintos cálculos de distancia, solo para ver las diferencias
dd2 = distance_matrix(data[movies], data[movies], p=2)
dd10 = distance_matrix(data[movies], data[movies], p=10)

def dm_to_df(dd, col_name): #Para pasar los resultados a dataframes
    import pandas as pd
    return pd.DataFrame(dd, index=col_name, columns=col_name)

dm_to_df(dd1, data["user_id"])
dm_to_df(dd2, data["user_id"])
dm_to_df(dd10, data["user_id"])

fig = plt.figure() #Graficar los datos de la tabla en 3D
ax = fig.add_subplot(111, projection="3d")
ax.scatter(xs=data["star_wars"], ys=data["lord_of_the_rings"], zs = data["harry_potter"])

#CREAR A MANO EL PROCESO DEL CLUSTERING.. (SOLO DE EJEMPLO PARA ENTENDER LO QUE PASA)
df = dm_to_df(dd1, data["user_id"])
z = [] #Para añadir los grupos y a qué distancia

df[11] = df[1] + df[10]
df.loc[11] = df.loc[1] + df.loc[10]
z.append([1,10, 0.7, 2]) #Puntos que vamos a juntar, la distancia, y la cantidad de elementos en el cluster

for i in df.columns.values.tolist(): #Recorrer el df y agregar la fila 11 con los valores sw la mínima distancia con las columnas juntadas
    df.loc[11][i] = min(df.loc[1][i], df.loc[10][i])
    df.loc[i][11] = min(df.loc[i][1], df.loc[i][10])

df = df.drop([1,10]) #Eliminar las filas y columnas que se juntaron antes (1 y 10)
df = df.drop([1,10], axis = 1)

#VOLVER A REPETIR EL PASO ANTERIOR.. 2da
x = 2
y = 7
n = 12

df[n] = df[x] + df[y]
df.loc[n] = df.loc[x] + df.loc[y]
z.append([x,y, df.loc[x][y], 2]) #Puntos que vamos a juntar, la distancia, y la cantidad de elementos en el cluster

for i in df.columns.values.tolist(): #Recorrer el df y agregar la fila 11 con los valores sw la mínima distancia con las columnas juntadas
    df.loc[n][i] = min(df.loc[x][i], df.loc[y][i])
    df.loc[i][n] = min(df.loc[i][x], df.loc[i][y])

df = df.drop([x,y]) #Eliminar las filas y columnas que se juntaron antes (1 y 10)
df = df.drop([x,y], axis = 1)

#VOLVER A REPETIR EL PASO ANTERIOR.. 3ra
x = 5
y = 8
n = 13

df[n] = df[x] + df[y]
df.loc[n] = df.loc[x] + df.loc[y]
z.append([x,y, df.loc[x][y], 2]) #Puntos que vamos a juntar, la distancia, y la cantidad de elementos en el cluster

for i in df.columns.values.tolist(): #Recorrer el df y agregar la fila 11 con los valores sw la mínima distancia con las columnas juntadas
    df.loc[n][i] = min(df.loc[x][i], df.loc[y][i])
    df.loc[i][n] = min(df.loc[i][x], df.loc[i][y])

df = df.drop([x,y]) #Eliminar las filas y columnas que se juntaron antes (1 y 10)
df = df.drop([x,y], axis = 1)

#VOLVER A REPETIR EL PASO ANTERIOR.. 4to
x = 11
y = 13
n = 14

df[n] = df[x] + df[y]
df.loc[n] = df.loc[x] + df.loc[y]
z.append([x,y, df.loc[x][y], 2]) #Puntos que vamos a juntar, la distancia, y la cantidad de elementos en el cluster

for i in df.columns.values.tolist(): #Recorrer el df y agregar la fila 11 con los valores sw la mínima distancia con las columnas juntadas
    df.loc[n][i] = min(df.loc[x][i], df.loc[y][i])
    df.loc[i][n] = min(df.loc[i][x], df.loc[i][y])

df = df.drop([x,y]) #Eliminar las filas y columnas que se juntaron antes (1 y 10)
df = df.drop([x,y], axis = 1)

#VOLVER A REPETIR EL PASO ANTERIOR.. 5to
x = 9
y = 12
m = 14
n = 15

df[n] = df[x] + df[y]
df.loc[n] = df.loc[x] + df.loc[y]
z.append([x,y, df.loc[x][y], 3]) #Puntos que vamos a juntar, la distancia, y la cantidad de elementos en el cluster

for i in df.columns.values.tolist(): #Recorrer el df y agregar la fila 11 con los valores sw la mínima distancia con las columnas juntadas
    df.loc[n][i] = min(df.loc[x][i], df.loc[y][i], df.loc[m][i])
    df.loc[i][n] = min(df.loc[i][x], df.loc[i][y], df.loc[i][m])

df = df.drop([x,y,m]) #Eliminar las filas y columnas que se juntaron antes (1 y 10)
df = df.drop([x,y,m], axis = 1)

#VOLVER A REPETIR EL PASO ANTERIOR.. 6to
x = 4
y = 6
m = 15
n = 16

df[n] = df[x] + df[y]
df.loc[n] = df.loc[x] + df.loc[y]
z.append([x,y, df.loc[x][y], 3]) #Puntos que vamos a juntar, la distancia, y la cantidad de elementos en el cluster

for i in df.columns.values.tolist(): #Recorrer el df y agregar la fila 11 con los valores sw la mínima distancia con las columnas juntadas
    df.loc[n][i] = min(df.loc[x][i], df.loc[y][i], df.loc[m][i])
    df.loc[i][n] = min(df.loc[i][x], df.loc[i][y], df.loc[i][m])

df = df.drop([x,y,m]) #Eliminar las filas y columnas que se juntaron antes (1 y 10)
df = df.drop([x,y,m], axis = 1)

#VOLVER A REPETIR EL PASO ANTERIOR.. 7mo
x = 3
y = 16
n = 17

df[n] = df[x] + df[y]
df.loc[n] = df.loc[x] + df.loc[y]
z.append([x,y, df.loc[x][y], 2]) #Puntos que vamos a juntar, la distancia, y la cantidad de elementos en el cluster

for i in df.columns.values.tolist(): #Recorrer el df y agregar la fila 11 con los valores sw la mínima distancia con las columnas juntadas
    df.loc[n][i] = min(df.loc[x][i], df.loc[y][i])
    df.loc[i][n] = min(df.loc[i][x], df.loc[i][y])

df = df.drop([x,y]) #Eliminar las filas y columnas que se juntaron antes (1 y 10)
df = df.drop([x,y], axis = 1)


# == CLUSTERING UTILIZANDO PYTHON ==
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage #linkage para hacer el cluster anterior con python

Z = linkage(data[movies], "ward") #El método de "ward" minimiza los cuadrados, es poco común usarlo
plt.figure(figsize=(25,10))
plt.title("Dendrograma jerárquico para el Clustering")
plt.xlabel("ID de los usuarios de Netflix")
plt.ylabel("Distancia")
dendrogram(Z, leaf_rotation=90.0, leaf_font_size=15.0, orientation="right") #leaf_rotation para girar los nombre en 90°, orientation para girarlo hacia la derecha..
plt.show()

Z = linkage(data[movies], "average") #El método de "average" es la distancia promedio entre los puntos
plt.figure(figsize=(25,10))
plt.title("Dendrograma jerárquico para el Clustering")
plt.xlabel("ID de los usuarios de Netflix")
plt.ylabel("Distancia")
dendrogram(Z, leaf_rotation=90.0, leaf_font_size=12.0) #leaf_rotation para girar los nombre en 90°, orientation para girarlo hacia la derecha..
plt.show()

Z = linkage(data[movies], "complete") #El método de "complete" toma las distancias más lejanas
plt.figure(figsize=(25,10))
plt.title("Dendrograma jerárquico para el Clustering")
plt.xlabel("ID de los usuarios de Netflix")
plt.ylabel("Distancia")
dendrogram(Z, leaf_rotation=90.0, leaf_font_size=12.0) #leaf_rotation para girar los nombre en 90°, orientation para girarlo hacia la derecha..
plt.show()

Z = linkage(data[movies], "simple") #El método de "simple" toma las distancias más cortas
plt.figure(figsize=(25,10))
plt.title("Dendrograma jerárquico para el Clustering")
plt.xlabel("ID de los usuarios de Netflix")
plt.ylabel("Distancia")
dendrogram(Z, leaf_rotation=90.0, leaf_font_size=12.0) #leaf_rotation para girar los nombre en 90°, orientation para girarlo hacia la derecha..
plt.show()

#PARA VER MÁS DETALLES DE TODO LO ANTERIOR, BUSCAR LINKAGE DE PYTHON.. https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html


# == CLUSTERING JERÁRQUICO ==
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet
from scipy.spatial.distance import pdist
import numpy as np

np.random.seed(4711)
a = np.random.multivariate_normal([10,0], [[3,1], [1,4]], size = [100,])
b = np.random.multivariate_normal([0,20], [[3,1], [1,4]], size = [50,])
X = np.concatenate((a,b))
print(X.shape)
plt.scatter(X[:,0], X[:,1])
plt.show()

Z = linkage(X, "ward") #se pueden usar distintos tipos de métricas, entregando otro parámetro metrics="...."
c, coph_dist = cophenet(Z, pdist(X)) #Entrega la certeza del resultado..

idx = [33,62,68]
idx2 = [15,69.41]
plt.figure(figsize = (10,8))
plt.scatter(X[:,0], X[:,1]) #Pintar todos los puntos
plt.scatter(X[idx,0], X[idx,1], c='r') #Destacar en rojo los puntos que nos interesesan ver del gráfico..
plt.scatter(X[idx2,0], X[idx2,1], c='y') #Destacar en amarillo los puntos que nos interesesan ver del gráfico..
plt.show()

plt.figure(figsize=(25,10))
plt.title("Dendrograma del clustering jerárquico")
plt.xlabel("Índices de la Muestra")
plt.ylabel("Distancias")
dendrogram(Z, leaf_rotation=90., leaf_font_size=8.0) #Se puede agregar el nombre de los labels, "labels = ......." y también el color_threshold = "límite" para que pinte de otro color después de X número..
plt.show

#TRUNCAR ELEMENTOS, PARA EL CASO DE QUE APAREZCAN DISTANCIAS MUY GRANDES..
plt.figure(figsize=(25,10))
plt.title("Dendrograma del clustering jerárquico")
plt.xlabel("Índices de la Muestra")
plt.ylabel("Distancias")
dendrogram(Z, leaf_rotation=90., leaf_font_size=8.0, color_threshold = 0.7*180, 
           truncate_mode = "lastp", p = 10, show_leaf_counts = False, show_contracted = True) #SOLO MUESTRA LAS "p" ÚLTIMAS UNIONES.. EL RESTO LAS JUNTA..
plt.show #show_leaf_counts = True, VA A INDICAR LA CANTIDAD DE ELEMENTOS QUE SE JUNTARON EN EL CLUSTER INDICADO..

def dendrogram_tune(*args, **kwargs): #FUNCIÓN PARA GRAFICAR EL DENDROGRAMA, AGRUPADO, CON COLORES y LA INFORMACIÓN QUE NOS INTERESA..
    max_d = kwargs.pop("max_d", None)
    if max_d and 'color_threshold' not in kwargs:
        kwargs['color_threshold'] = max_d
    annotate_above = kwargs.pop('annotate_above', 0)
    
    ddata = dendrogram(*args,**kwargs)
    
    if not kwargs.get('no_plot', False):
        plt.title("Clustering jerárquico con Dendrograma truncado")
        plt.xlabel("Índice del Dataset (o tamaño del cluster)")
        plt.ylabel("Distancia")
        for index, distance, color in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):
            x = 0.5*sum(index[1:3])
            y = distance[1]
            if y > annotate_above:
                plt.plot(x,y, 'o', c=color)
                plt.annotate('%.3g'%y, (x,y), xytext = (0,-5), textcoords = "offset points", va = "top", ha = "center")
                
    if max_d:
        plt.axhline(y=max_d, c='k')
        
    return ddata

dendrogram_tune(Z, truncate_mode='lastp', p = 12, leaf_rotation=90., leaf_font_size = 12., show_contracted=True, annotate_above = 10, max_d = 50)


#MÉTODO PARA ELEGIR LA CANTIDAD DE CLUSTERS QUE SE DEBERÍAN FORMAR:
from scipy.cluster.hierarchy import inconsistent

depth = 5 #Cantidad de niveles de profundidad/por debajo del cluster calculado
incons = inconsistent(Z, depth) #Entrega el promedio, Desv. Std, Cuenta, Factor de Inconsistencia

#MÉTODO DEL CODO:
last = Z[-10:,2]
last_rev = last[::-1] #REVERTIR LOS DATOS
idx = np.arange(1,len(last)+1)
plt.plot(idx, last_rev)

acc = np.diff(last,2)
acc_rev = acc[::-1] 
plt.plot(idx[:-2]+1, acc_rev)
plt.show()
k = acc_rev.argmax()+2
print("El número óptimo de cluster es %s"%str(k))


#EJEMPLO DONDE NO FUNCIONA HACER SOLO 2 CLUSTER:
c = np.random.multivariate_normal([40,40], [[20,1], [1,30]], size = [200,])
d = np.random.multivariate_normal([80,80], [[30,1], [1,30]], size = [200,])
e = np.random.multivariate_normal([0,100], [[100,1], [1,100]], size = [200,])    
X2 = np.concatenate((X,c,d,e),)
plt.scatter(X2[:,0], X2[:,1])
plt.show()

Z2 = linkage(X2, "ward")
plt.figure(figsize=(10,10))
dendrogram_tune(
    Z2,
    truncate_mode = "lastp",
    p = 30,
    leaf_rotation=90.,
    leaf_font_size = 10.,
    show_contracted = True,
    annotate_above = 40,
    max_d = 170
    )
plt.show()

last = Z2[-10:,2]
last_rev = last[::-1] #REVERTIR LOS DATOS
idx = np.arange(1,len(last)+1)
plt.plot(idx, last_rev)

acc = np.diff(last,2)
acc_rev = acc[::-1] 
plt.plot(idx[:-2]+1, acc_rev)
plt.show()
k = acc_rev.argmax()+2
print("El número óptimo de cluster es %s"%str(k))

#CONCLUSIÓN: A SIMPLE VISTA PARECIERA QUE HAY QUE HACER 5 CLUSTER, PERO EL MÉTODO DEL CODO INDICA QUE EL ÓPTIMO ES 4, Y JUNTAR 2 GRUPOS..


#RECUPERAR CLUSTERS Y SUS ELEMENTOS
from scipy.cluster.hierarchy import fcluster

max_d = 20
clusters = fcluster(Z, max_d, criterion = "distance") #INDICA EN QUÉ CLUSTER QUEDA EL DATO DEPENDIENDO EL CRITERIO MÁXIMO..

k = 3
cluster = fcluster(Z, k, criterion="maxclust") #INDICA EN QUÉ CLUSTER QUEDA EL DATO DEPENDIENDO DE LA CANTIDAD DE CLUSTERS QUE QUIERO..

fcluster(Z, 8, depth=10) #SE LE ENTREGA LA CANTIDAD MÍNIMA DE ELEMENTOS QUE DEBE TENER EL CLUSTER (8) y LA PROFUNDIDAD A MIRAR..

plt.figure(figsize=(10,8))
plt.scatter(X[:,0], X[:,1], c=cluster, cmap="prism")
plt.show()




# == CLUSTERING CON K-MEANS ==
# Hay que elegir antes del análisis la cantidad de clusters, se intenta ir clasificado los datos según los centroides estimados..
import numpy as np

data = np.random.random(90).reshape(30,3)
c1 = np.random.choice(range(len(data))) #Se eligen 2 centroides al azar..
c2 = np.random.choice(range(len(data)))
clust_centers = np.vstack([data[c1], data[c2]])

from scipy.cluster.vq import vq

vq(data, clust_centers) #Primer arreglo indica la clasificación del dato, en el otro entrega la distancia al centroide del cluster asignado..

from scipy.cluster.vq import kmeans

kmeans(data, clust_centers)
kmeans(data, 2) #Asigna a la cantidad de clusters elegidos, en vez de tomar los que les dimos..



# == CLUSTERING CON DATOS DEL DATASET ==
# Datos de vinos tinto, donde entrega distintas características como la calidad de este..
import pandas as pd
import matplotlib.pyplot as plt

RUTA_CLASE_HOME = "D:/Python/Github/Curso UDemy/python-ml-course/datasets"
df = pd.read_csv(RUTA_CLASE_HOME + "/wine/winequality-red.csv", sep = ";")

plt.hist(df["quality"]) #Ver la cantidad de vinos que tiene cada nota..
df.groupby("quality").mean() #Tomar las primeras conclusiones y como podrían afectar las variables a la calidad del Vino..

#Normalización de los Datos: Para que tengan pesos similares y no afecte su tamaño..
df_norm = (df-df.min())/(df.max()-df.min()) 

#Clustering jerárquico con scikit-learn:
from sklearn.cluster import AgglomerativeClustering

clus = AgglomerativeClustering(n_clusters=6, linkage = "ward").fit(df_norm)
md_h = pd.Series(clus.labels_) #Para poner las Etiquetas..
plt.hist(md)
plt.title("Histograma de los clusters")
plt.xlabel("Cluster")
plt.ylabel("Número de vinos del cluster")

clus.children_ #Estructura de los hijos para ver como se arma el árbol

#Clustering con Dendrograma
from scipy.cluster.hierarchy import dendrogram, linkage

Z = linkage(df_norm, "ward")
plt.figure(figsize=(25,10))
plt.title("Dendrograma de los vinos")
plt.xlabel("ID del vino")
plt.ylabel("Distancia")
dendrogram(Z, leaf_rotation=90., leaf_font_size =8.)
plt.show()

#Clustering con K-Means
from sklearn.cluster import KMeans
from sklearn import datasets

model = KMeans(n_clusters = 6) #Indicar la cantidad de clasificaciones
model.fit(df_norm)
model.labels_ #Ver en qué grupo queda..
md_k = pd.Series(model.labels_)

df_norm["clust_h"] = md_h #Podemos ver que tienen a quedar clasificados en el mismo cluster, los "2" pasan a ser "3" del otro grupo..
df_norm["clust_k"] = md_k

model.cluster_centers_
model.intertia_ #Suma de los valores al cuadrado..

#Interpretación final:
df_norm.groupby("clust_k").mean() #Se puede elegir cualquiera de los dos clusters..


#Ajustar los parámetros del clustering, o la cantidad de grupos que se van a elegir..
#2 Métodos: Codo y Silueta
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn import metrics
from scipy.spatial.distance import cdist
from sklearn.metrics import silhouette_score, silhouette_samples

x1 = np.array([3,1,1,2,1,6,6,6,5,6,7,8,9,8,9,9,8])
x2 = np.array([5,4,5,6,5,8,6,7,6,7,1,2,1,2,3,2,3])
X = np.array(list(zip(x1,x2))).reshape(len(x1),2)

plt.plot()
plt.xlim([0,10])
plt.ylim([0,10])
plt.title("Dataset a clasificar")
plt.xlabel("x")
plt.ylabel("y")
plt.scatter(x1,x2)
plt.show()

max_k = 10 #Máximo número de iteraciones
K = range(1,max_k)
ssw = []
color_palette = [plt.cm.Spectral(float(i)/max_k) for i in K]
centroid = [sum(X)/len(X) for i in K]
sst = sum(np.min(cdist(X, centroid, "euclidean"), axis = 1))

for k in K:
    kmeanModel = KMeans(n_clusters = k).fit(X)
    
    centers = pd.DataFrame(kmeanModel.cluster_centers_) #Centros
    labels = kmeanModel.labels_ #Etiqueta
   
    ssw_k = sum(np.min(cdist(X, kmeanModel.cluster_centers_, "euclidean"), axis = 1)) #Distancia con el centro..
    ssw.append(ssw_k)
   
    label_color = [color_palette[i] for i in labels]
    
    #Fabricaremos una silueta para cada cluster.. pero se deja fuera k = 1 y k = len(X)
    if 1<k<len(X):
        #Crear un subplot de una fila y dos columnas
        fig, (axis1,axis2) = plt.subplots(1,2)
        fig.set_size_inches(20,8)
        
        #El primer subplot contendrá la silueta que puede tener valores desde -1 a 1
        axis1.set_xlim([-0.1, 1.0])
        #El número de clusters a insertar determinará el tamaño de cada barra, por lo que es importante el coeficiente (n_clusters+1)*10
        #el que será el espacio en blanco entre los clusters para separarlas..
        axis1.set_ylim([0, len(X)+(k+1)*10])
        
        silhouette_avg = silhouette_score(X, labels)
        print("* Para k = ",k, " el promedio de la silueta es de: ",silhouette_avg)
        sample_silhouette_values = silhouette_samples(X, labels)
        
        y_lower = 10
        for i in range(k):
            #Agregamos la silueta del cluster i-ésimo
            ith_cluster_sv = sample_silhouette_values[labels == i]
            print("    - Para i = ",i+1, " la silueta del cluster vale: ", np.mean(ith_cluster_sv))
            
            #Ordenamos descendientemente las siluetas del cluster i-ésimo..
            ith_cluster_sv.sort()
            
            #Calculamos donde colocar la primera silueta en el eje vertical
            ith_cluster_size = ith_cluster_sv.shape[0]
            y_upper = y_lower + ith_cluster_size
            
            #Elegimos el color del cluster
            color = color_palette[i]
            
            #Pintamos la silueta del cluster i-ésimo
            axis1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sv, facecolor = color, alpha = 0.7)
            
            #Etiquetamos dicho cluster con el número en el centro
            axis1.text(-0.05, y_lower + 0.5*ith_cluster_size, str(i+1))
            
            #Calculamos el nuevo y_lower para el siguiente cluster del gráfico
            y_lower = y_upper + 10 #Este 10 corresponde al del set_ylim... *10 <--
            
        axis1.set_title("Representación de la silueta para k = %s"%str(k))
        axis1.set_xlabel("S(i)")
        axis1.set_ylabel("ID del Cluster")
        
        #Fin de la representación de la silueta
    
    #Plot de los K-means con los puntos respectivos
    plt.plot()
    plt.xlim([0,10])
    plt.ylim([0,10])
    plt.title("Clustering para k = %s"%str(k))
    plt.scatter(x1,x2, c=label_color)
    plt.scatter(centers[0], centers[1], c=color_palette, marker = "x") #HAY QUE ARREGLAR EL C, TIRA ERROR......
    plt.show()
    

#REPRESENTACIÓN DEL CODO
plt.plot(K, ssw, "bx-")
plt.xlabel("k")
plt.ylabel("SSw(k)")
plt.title("La técnica del codo para encontrar el k óptimo")
plt.show()

#REPRESENTACIÓN DEL CODO NORMALIZADO
plt.plot(K, 1-ssw/sst, "bx-")
plt.xlabel("k")
plt.ylabel("1-norm(SSw(k))")
plt.title("La técnica del codo normalizado para encontrar el k óptimo")
plt.show()


#CLUSTERING POR PROPAGACIÓN DE LA AFINIDAD:
from sklearn.cluster import AffinityPropagation
from sklearn import metrics
from sklearn.datasets._samples_generator import make_blobs
import matplotlib.pyplot as plt
from itertools import cycle

centers = [[1,1], [-1,-1], [1,-1]]
X, labels = make_blobs(n_samples = 300, centers = centers, cluster_std=0.5, random_state=0)

plt.scatter(X[:,0], X[:,1], c = labels, s= 10, cmap="autumn") #EL "s" ES PARA EL TAMAÑO DE LOS PUNTOS..

'''
af = AffinityPropagation(preference=-50).fit(X)
cluster_center_ids = af.cluster_centers_indices_
labels = af.labels_
n_clust = len(cluster_center_ids)
SE DEJÓ DENTRO DE LA FUNCIÓN..
'''

def report_affinity_propagation(X):
    af = AffinityPropagation(preference=-50).fit(X)
    cluster_center_ids = af.cluster_centers_indices_
    clust_labels = af.labels_
    n_clust = len(cluster_center_ids)
    
    print("Número estimado de clusters: %d"%n_clust)
    print("Homogeneidad: %0.3f"%metrics.homogeneity_score(labels, clust_labels))
    print("Completitud: %0.3f"%metrics.completeness_score(labels, clust_labels))
    print("V-measure: %0.3f"%metrics.v_measure_score(labels, clust_labels))
    print("R2 ajustado: %0.3f"%metrics.adjusted_rand_score(labels, clust_labels))
    print("Información mútua ajustada: %0.3f"%metrics.adjusted_mutual_info_score(labels, clust_labels))
    print("Coeficiente de la silueta: %0.3f"%metrics.silhouette_score(X, labels, metric="sqeuclidean"))

    plt.figure(figsize=(16,9))
    plt.clf()
    
    colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
    for k, col in zip(range(n_clust), colors):
        class_members = (clust_labels == k)
        clust_center = X[cluster_center_ids[k]]
        plt.plot(X[class_members, 0], X[class_members, 1], col + '.')
        plt.plot(clust_center[0], clust_center[1], 'o', markerfacecolor = col, markeredgecolor='k', markersize=14)
        for x in X[class_members]:
            plt.plot([clust_center[0], x[0]], [clust_center[1], x[1]], col)
        
    plt.title("Número estimado de clusters %d"%n_clust)
    plt.show()
        
report_affinity_propagation(X)


# DISTRIBUCIONES EN FORMA DE ANILLO
from math import sin, cos, radians, pi, sqrt
import numpy.random as rnd
import numpy as np
import matplotlib.pyplot as plt

def ring(r_min = 0, r_max = 1, n_samples = 360):
    angle = rnd.uniform(0,2*pi, n_samples)
    distance = rnd.uniform(r_min, r_max, n_samples)
    data = []
    for a,d in zip(angle,distance):
        data.append([d*cos(a), d*sin(a)]) #También se puede usar la raíz cuadrada de las distancias para que queden más cerca

    return np.array(data)

data1 = ring(3,5)
data2 = ring(24,27)

data = np.concatenate([data1, data2], axis = 0)
labels = np.concatenate([[0 for i in range(0,len(data1))], [1 for i in range(0, len(data2))]])

plt.scatter(data[:,0], data[:,1], c=labels, s=5, cmap = "autumn")

#ALGORITMNO CON KMEANS..
from sklearn.cluster import KMeans

km = KMeans(2).fit(data)
clust = km.predict(data)
plt.scatter(data[:,0], data[:,1], c= clust, s=5, cmap = "autumn") 
#Acá nos damos cuenta que KMeans no puede clasificar como anillos, intenta hacer círculos de acuerdo a las distancias de los puntos..


#PARA SOLUCIONAR LO ANTERIOR VAMOS A UTILIZAR K-MEDOIDES
from pyclust import KMedoids #pip install pyclust, pip install treelib

kmed = KMedoids(2).fit_predict(data)
plt.scatter(data[:,0], data[:,1], c=kmed, s=5, cmap="autumn")

#Podemos concluir que no nos sirve KMedoids, no hay mejoras en la separación de clusters..

#ALGORITMO DEL CLUSTERING ESPECTRAL:
#Utiliza un algoritmo de acuerdo a una fórmula de distancias exponenciales, y arma un espectro para llegar al mejor resultado..
from sklearn.cluster import SpectralClustering

clust = SpectralClustering(2).fit_predict(data)
plt.scatter(data[:,0], data[:,1], c=clust, s=5, cmap="autumn")

'''
Queda mucho mejor el cluster, pero lamentablemente no se puede determinar la K.
Siempre tendremos que empezar con propagación para revisar la cantidad de K a usar..

*Podemos estimar la k?
    *NO: Podemos utilizar propagación de la afinidad
    *SI: Podemos utilizar la distancia Euclídea?
        *SI: Utilizar K-Medoides
        *NO: Los datos son linealmente separables?
            *SI: Utilizar Clustering Aglomerativo
            *NO: Utilizar Clustering Espectral
'''






# ============== SECCIÓN 10 ==================

# === ÁRBOLES DE DECISIÓN ====
'''
Árbol de decisión se utiliza cuando la respuesta es categórica/discreta y se puede clasificar.
Bosques aleatorios, para juntar varios árboles de decisión y mejorar la conclusión.
Entropía: número mínimo de bits para codificar una clasificación.
Algoritmo ID3: buscar la máxima entropia (nula) y repetir este paso hasta llegar al mejor resultado.
Índice de Gini: funciona cuando las respuestas son binarias. Se busca el valor más alto.
Reducción de la varianza: Buscar la menor posible, ya que tiene los datos más concentrados.
Poda del árbol: definir nivel de error y eliminar ramas hasta llegar al objetivo.
'''

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.tree import DecisionTreeClassifier

RUTA_CLASE_HOME = "D:/Python/Github/Curso UDemy/python-ml-course/datasets"
data = pd.read_csv(RUTA_CLASE_HOME + "/iris/iris.csv")

colnames = data.columns.values.tolist()
predictors = colnames[:4]
target = colnames[4]

data["is_train"] = np.random.uniform(0,1, len(data))<=0.75 #Generar distribución uniforme entre 0 y 1, y asignar una parte para para pruebas y otras para trabajar
train, test = data[data["is_train"]==True], data[data["is_train"]==False] #Asignar los datos a las distintas variables
              
tree = DecisionTreeClassifier(criterion="entropy", min_samples_split=20, random_state=99) #min_sample_leaf es para indicar que los nodos no tengan menos de X elementos
tree.fit(train[predictors], train[target])                   

preds = tree.predict(test[predictors])
pd.crosstab(test[target], preds, rownames = ["Actual"], colnames = ["Predictions"]) #Hay 2 que se clasificaron mal, las que están distintas en la tabla..


# VISUALIZACIÓN DEL ARBOL DE DECISIÓN:
from sklearn.tree import export_graphviz
with open("resources/iris_dtree.dot", "w") as dotfile: #HAY QUE ARREGLAR DONDE SE ENCUENTRA EL FICHERO..
    export_graphviz(tree, out_file=dotfile, feature_names=predictors)
    dotfile.close()

import os
from graphviz import Source #conda install graphviz

file = open("resources/iris_dtree.dot", "r")
text = file.read()
Source(text)


# TÉCNICA DE VALIDACIÓN CRUZADA / CROSS VALIDATION PARA LA PODA
X = data[predictors]
Y = data[target]
tree = DecisionTreeClassifier(criterion="entropy", max_depth = 5, min_samples_split = 20, random_state = 99)
tree.fit(X,Y)

from sklearn.model_selection import KFold
cv = KFold( n_splits=10, shuffle = True, random_state=1) 

from sklearn.model_selection import cross_val_score
score = np.mean(cross_val_score(tree, X, Y, scoring="accuracy", cv = cv, n_jobs=1))
print(score) #SCORE DE 0,9333

for i in range(1,11):
    tree = DecisionTreeClassifier(criterion="entropy", max_depth = i, min_samples_split = 20, random_state = 99)
    tree.fit(X,Y)
    cv = KFold( n_splits=10, shuffle = True, random_state=1) 
    scores = cross_val_score(tree, X, Y, scoring="accuracy", cv = cv, n_jobs=1)
    score = np.mean(scores)
    print("Score para i =",i, "es de ", score)
    print("   ", tree.features_importances_)
#SE PUEDE CONCLUIR QUE EL MÁXIMO SE ALCANZA EN PROFUNDIDAD 3, Y SOBRE ESTO SE MANTIENE CONSTANTE EN 0,933
#DENTRO DE LOS RESULTADOS DE features, INDICA QUE predictors[2] (Petal.Length) ES LA MÁS IMPORTANTE, seguido por el último..
